{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Lab No 10</h4>\n",
        "<h4>Utkarsh Bhangale<br>2020802124<br>Data Science</h4>"
      ],
      "metadata": {
        "id": "NFt0QzxEV-Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim -** Program to solve AI Fairness issue."
      ],
      "metadata": {
        "id": "mFcbSxh9WT25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Software Required:**\n",
        "Jupyter Notebook, IDLE, Any Python Editor"
      ],
      "metadata": {
        "id": "eArLJbaMWeEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prerequisite:**\n",
        "Familiarity with bias & fairness concepts, pandas,Python syntax, ML datasets/workflow, basic debiasing"
      ],
      "metadata": {
        "id": "Dl_Lv9OfWi-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory/concept:**<br>\n",
        "Algorithmic Fairness: The goal is to develop models that are fair and avoid discrimination or disparate treatment/impact based on sensitive attributes like gender, race etc.\n",
        "\n",
        "Demographic Parity: Used as a fairness metric to measure unfair differences in predictive outcomes across demographic groups. Ensures equal true positive/negative rates.\n",
        "\n",
        "Fairness Constraints: Algorithms like Exponentiated Gradient optimize models to directly satisfy fairness constraints like demographic parity during training.\n",
        "\n",
        "Disparate Impact/Treatment: Fairness metrics quantify initial unfairness in the model to check for differences in predictions or decisions that disadvantage protected groups.\n",
        "\n",
        "Mitigation Techniques: Debiasing algorithms aim to modify model training to reduce unfairness while maintaining high accuracy. This tries to resolve unfairness problems.\n",
        "\n",
        "Potential Bias in Data: Synthetic data is used to control for biases in real data collection/labeling processes and isolate effects of modeling choices on fairness.\n",
        "\n",
        "Fair Machine Learning workflow: Stages involve defining metrics, auditing initial unfairness, using specialized techniques during training, and re-checking fairness post debiasing.\n",
        "\n",
        "Explainable/Transparent Systems: Goals like interpretability and accountability promote developing fair models through techniques users and auditors can understand.\n",
        "\n",
        "Ongoing Monitoring: Fairness is not a single evaluation but requires regular re-assessment over time as data and environments change."
      ],
      "metadata": {
        "id": "t5725NTkWyqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algorithm/Pseudo code:**<br>\n",
        "\n",
        "**Data Generation:**<br>\n",
        "Define features/attributes like gender, race, skills etc<br>\n",
        "Initialize empty dictionary/list to store values<br>\n",
        "Generate random values for each attribute<br>\n",
        "Populate dictionary with feature-value pairs<br>\n",
        "Convert dictionary to DataFrame<br><br>\n",
        "**Bias Measurement:**<br>\n",
        "Filter rows by gender and check if hired = Yes<br>\n",
        "Count males and females matching this criteria<br>\n",
        "Calculate ratio of hired males to total males\n",
        "Similarly for females<br>\n",
        "Calculate disparate impact as ratio of ratios<br>\n",
        "Repeat above steps for race categories\n",
        "<br><br>\n",
        "**Reweighting:**\n",
        "Calculate total counts for each gender/race<br>\n",
        "Calculate weights as ratio of underrepresented to overrepresented group counts<br>\n",
        "Initialize weight column in DataFrame<br>\n",
        "Filter rows by gender/race and assign calculated weight<br><br>\n",
        "**Repeat steps 2-3 on reweighted data to validate mitigation of bias**<br><br>\n",
        "**Pseudo-code:**\n",
        "\n",
        "GenerateData()<br>\n",
        "FilterRows(column1, condition1)<br>\n",
        "CountMatches(column1, condition1, condition2)<br>\n",
        "CalculateRatio(num, den)<br>\n",
        "CalculateDisparateImpact(ratio1, ratio2)<br>\n",
        "CalculateWeights(count1, count2)<br>\n",
        "AssignWeights(column1, weight)<br>\n",
        "ValidateReducedBias()<br>"
      ],
      "metadata": {
        "id": "-wu1bAgYW81d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relative Applications -**<br>\n",
        "Credit scoring/lending - Developing unbiased risk assessment models for loan approvals.\n",
        "\n",
        "Criminal justice - Implementing fair algorithms in areas like risk assessment, bail/sentencing.\n",
        "\n",
        "Employment/hiring - Building equitable recruitment/hiring systems that avoid discrimination.\n",
        "\n",
        "Education - Creating inclusive personalized learning and student support tools.\n",
        "\n",
        "Healthcare - Applying fair ML to disease risk prediction, treatment recommendations, insurance approvals etc.\n",
        "\n",
        "Smart cities - Ensuring algorithmic decision systems for infrastructure/services don't negatively impact disadvantaged groups.\n",
        "\n",
        "Autonomous vehicles - Developing self-driving car AI that considers safety and fairness for all road users.\n",
        "\n",
        "Customer service - Building chatbots, support systems etc that are respectful and responsive to all customers regardless of personal attributes.\n",
        "\n",
        "Marketing/advertising - Developing fair targeted advertising models that don't enable digital redlining or hidden biases.\n",
        "\n",
        "Government services - Implementing fairness best practices in computational processes that directly impact citizens like welfare programs, public transportation etc.\n",
        "\n",
        "Policing - Helping develop explainable, transparent and equitable tools to support public safety operations."
      ],
      "metadata": {
        "id": "8e7Rc3oeXNgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aif360"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vLeIIlbpW00E",
        "outputId": "537321f4-314a-4660-ab6e-b7e072214f45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aif360\n",
            "  Downloading aif360-0.5.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.1/214.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.11.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from aif360) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.0->aif360) (1.16.0)\n",
            "Installing collected packages: aif360\n",
            "Successfully installed aif360-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate synthetic data for the job application dataset\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate features\n",
        "experience = np.random.randint(0, 20, n_samples)  # Years of experience\n",
        "education_level = np.random.randint(1, 5, n_samples)  # Education level (1-4)\n",
        "\n",
        "# Define the gender attribute (0: Female, 1: Male)\n",
        "gender = np.random.choice([0, 1], n_samples, p=[0.4, 0.6])\n",
        "\n",
        "# Simulate the interview invitation status (0: Not invited, 1: Invited)\n",
        "interview_invitation = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Experience': experience,\n",
        "    'Education_Level': education_level,\n",
        "    'Gender': gender,\n",
        "    'Interview_Invitation': interview_invitation\n",
        "})\n",
        "\n",
        "# Save the synthetic dataset to a CSV file\n",
        "data.to_csv('job_applications_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Mi0d0IzfXZmW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', FutureWarning)\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Experience': experience,\n",
        "    'Education_Level': education_level,\n",
        "    'Gender': gender,\n",
        "    'Interview_Invitation': interview_invitation\n",
        "})\n",
        "\n",
        "# Identify the sensitive attribute (Gender) and the target variable.\n",
        "sensitive_attr = 'Gender'\n",
        "target_attr = 'Interview_Invitation'\n",
        "\n",
        "# Split the data into features (X) and the target variable (y).\n",
        "X = data.drop(target_attr, axis=1)\n",
        "y = data[target_attr]\n",
        "\n",
        "# Split the data into training and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Train a baseline model without considering fairness (e.g., logistic regression).\n",
        "baseline_model = LogisticRegression()\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the baseline model.\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "\n",
        "# Define the protected attribute (Gender) and privileged group (e.g., Male).\n",
        "protected_attr = 'Gender'\n",
        "privileged_group = [{'Gender': 1}]  # Replace with the privileged group definition in your data\n",
        "\n",
        "# Create a BinaryLabelDataset to perform fairness assessments.\n",
        "dataset = BinaryLabelDataset(\n",
        "    favorable_label=1,\n",
        "    unfavorable_label=0,\n",
        "    df=data,\n",
        "    label_names=['Interview_Invitation'],\n",
        "    protected_attribute_names=[protected_attr],\n",
        ")\n",
        "\n",
        "# Implement reweighing to mitigate bias.\n",
        "RW = Reweighing(unprivileged_groups=[{protected_attr: 0}], privileged_groups=[{protected_attr: 1}])\n",
        "reweighted_dataset = RW.fit_transform(dataset)\n",
        "\n",
        "# Train a new hiring model using the reweighed data.\n",
        "fair_model = LogisticRegression()\n",
        "fair_model.fit(reweighted_dataset.features, reweighted_dataset.labels.ravel())\n",
        "\n",
        "# Make predictions using the fair model.\n",
        "y_pred_fair = fair_model.predict(X_test)\n",
        "\n",
        "# Evaluate fairness and performance metrics.\n",
        "metric_fair = BinaryLabelDatasetMetric(reweighted_dataset, unprivileged_groups=[{protected_attr: 0}],\n",
        "                                       privileged_groups=[{protected_attr: 1}])\n",
        "\n",
        "disparate_impact_fair = metric_fair.disparate_impact()\n",
        "\n",
        "statistical_parity_difference_fair = metric_fair.statistical_parity_difference()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_fair)\n",
        "precision = precision_score(y_test, y_pred_fair)\n",
        "recall = recall_score(y_test, y_pred_fair)\n",
        "f1 = f1_score(y_test, y_pred_fair)\n",
        "\n",
        "# Print fairness and performance metrics.\n",
        "print(\"Disparate Impact (Fair Model):\", disparate_impact_fair)\n",
        "\n",
        "print(\"Statistical Parity Difference (Fair Model):\", statistical_parity_difference_fair)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Lean8lnoXfCh",
        "outputId": "7a846746-8c8a-4d15-e3ef-981bd99acc98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disparate Impact (Fair Model): 1.0000000000000004\n",
            "Statistical Parity Difference (Fair Model): 1.6653345369377348e-16\n",
            "Accuracy: 0.725\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}